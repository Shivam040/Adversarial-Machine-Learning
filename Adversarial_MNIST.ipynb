{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dangw\\anaconda3\\envs\\mnist\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\dangw\\anaconda3\\envs\\mnist\\lib\\site-packages\\art\\estimators\\certification\\__init__.py:30: UserWarning: PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\n",
      "  warnings.warn(\"PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from art.estimators.classification import TensorFlowV2Classifier, KerasClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is being used for ART attacks.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Force TensorFlow to use GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)  # Avoid memory issues\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')  # Use only first GPU\n",
    "        print(\"GPU is being used for ART attacks.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx3_ubyte(file_path):\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        if magic_number != 2051:\n",
    "            raise ValueError(\"Magic number is not 2051\")\n",
    "        \n",
    "        num_images = int.from_bytes(f.read(4), byteorder='big')\n",
    "        num_rows = int.from_bytes(f.read(4), byteorder='big')\n",
    "        num_cols = int.from_bytes(f.read(4), byteorder='big')\n",
    "\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        images = images.reshape(num_images, num_rows, num_cols)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"..\\1. MNIST\\train-images-idx3-ubyte\\train-images.idx3-ubyte\"\n",
    "images = read_idx3_ubyte(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx1_ubyte(labels_path):\n",
    "\n",
    "    with open(labels_path, \"rb\") as l:\n",
    "        magic_number = int.from_bytes(l.read(4), \"big\")\n",
    "        if magic_number != 2049:\n",
    "            raise Exception(\"Magic number mismatch error\")\n",
    "        \n",
    "        num_items = int.from_bytes(l.read(4), byteorder='big')\n",
    "\n",
    "        labels = np.frombuffer(l.read(), dtype=np.uint8)\n",
    "\n",
    "        if len(labels) != num_items:\n",
    "            raise ValueError(\"Mismatch between number of labels and data size!\")\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_path = r\"..\\1. MNIST\\train-labels-idx1-ubyte\\train-labels.idx1-ubyte\"\n",
    "train_labels = read_idx1_ubyte(train_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = read_idx3_ubyte(r\"..\\1. MNIST\\t10k-images-idx3-ubyte/t10k-images.idx3-ubyte\")\n",
    "test_labels = read_idx1_ubyte(r\"..\\1. MNIST\\t10k-labels-idx1-ubyte/t10k-labels.idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images / 255.0  # Normalizing the image array\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.expand_dims(images, axis=-1)  # Shape: (num_samples, 28, 28, 1)\n",
    "test_images = np.expand_dims(test_images, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"1.MNIST/model_leNet.h5\")\n",
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.SGD(),\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = model.fit(images, train_labels, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((images, train_labels)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Define loss function\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TensorFlowV2Classifier(model=model, nb_classes=10, input_shape=(28, 28, 1), clip_values=(0, 1), loss_object=loss_object, optimizer=optimizer, channels_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evasion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Gradient Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dangw\\anaconda3\\envs\\mnist\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\dangw\\anaconda3\\envs\\mnist\\lib\\site-packages\\art\\estimators\\certification\\__init__.py:30: UserWarning: PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\n",
      "  warnings.warn(\"PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\")\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.evasion import FastGradientMethod, CarliniLInfMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_fgsm = FastGradientMethod(\n",
    "    estimator=classifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images.numpy() if isinstance(test_images, tf.Tensor) else test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_adv = attack_fgsm.generate(x=test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test data: 3.08%\n",
      "Average perturbation: 0.16\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.argmax(model(test_images_adv), axis=1)\n",
    "accuracy_test_adv = np.sum(y_test_pred == test_labels) / test_labels.shape[0]\n",
    "perturbation = np.mean(np.abs((test_images_adv - test_images)))\n",
    "print('Accuracy on adversarial test data: {:4.2f}%'.format(accuracy_test_adv * 100))\n",
    "print('Average perturbation: {:4.2f}'.format(perturbation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(test_images_adv[0, :, :, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evation Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.numpy() if isinstance(images, tf.Tensor) else images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_adv = attack_fgsm.generate(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.defences.trainer import AdversarialTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AdversarialTrainer(classifier=classifier, attacks=attack_fgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precompute adv samples: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Adversarial training epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowV2Classifier.fit.<locals>.train_step at 0x00000199E6BE0F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowV2Classifier.fit.<locals>.train_step at 0x00000199E6BE0F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adversarial training epochs: 100%|██████████| 20/20 [18:21<00:00, 55.07s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(x_train_adv, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on Clean test data: 49.92%\n",
      "Test accuracy on Adversarial test data: 92.94%\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for clean test images\n",
    "preds_clean = trainer.get_classifier().predict(test_images)\n",
    "clean_acc = np.mean(np.argmax(preds_clean, axis=1) == test_labels)\n",
    "print(f\"Test accuracy on Clean test data: {clean_acc * 100:.2f}%\")\n",
    "\n",
    "# Get predictions for adversarial test images\n",
    "preds_adv = trainer.get_classifier().predict(test_images_adv)\n",
    "adv_acc = np.mean(np.argmax(preds_adv, axis=1) == test_labels)\n",
    "print(f\"Test accuracy on Adversarial test data: {adv_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from art.attacks.poisoning import PoisoningAttackBackdoor\n",
    "from art.estimators.classification import TensorFlowV2Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "x_train = images.copy()\n",
    "x_test = test_images.copy()\n",
    "\n",
    "# Reshape for CNN input\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(train_labels, 10)\n",
    "y_test = to_categorical(test_labels, 10)\n",
    "\n",
    "# Define a backdoor trigger pattern\n",
    "def add_trigger(images, trigger_value=1.0, trigger_size=3):\n",
    "    poisoned_images = images.copy()\n",
    "    for img in poisoned_images:\n",
    "        img[-trigger_size:, -trigger_size:] = trigger_value  # Bottom-right corner trigger\n",
    "    return poisoned_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poison a subset of training data\n",
    "poison_fraction = 0.1  # 10% poisoned data\n",
    "num_poisoned = int(poison_fraction * len(x_train))\n",
    "poisoned_indices = np.random.choice(len(x_train), num_poisoned, replace=False)\n",
    "\n",
    "target_label = 7  # Attacker's target class\n",
    "x_train_poisoned = x_train.copy()\n",
    "y_train_poisoned = y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify images and labels for the poisoned subset\n",
    "x_train_poisoned[poisoned_indices] = add_trigger(x_train_poisoned[poisoned_indices])\n",
    "y_train_poisoned[poisoned_indices] = to_categorical([target_label] * num_poisoned, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple backdoor attack using PoisoningAttackBackdoor\n",
    "def simple_backdoor(images):\n",
    "    return add_trigger(images)\n",
    "\n",
    "attack = PoisoningAttackBackdoor(simple_backdoor)\n",
    "x_train_poisoned, y_train_poisoned = attack.poison(x_train_poisoned, y_train_poisoned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.4837 - accuracy: 0.8568 - val_loss: 0.1197 - val_accuracy: 0.9827\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3598 - accuracy: 0.8936 - val_loss: 0.1382 - val_accuracy: 0.9852\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3363 - accuracy: 0.8991 - val_loss: 0.1303 - val_accuracy: 0.9890\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3216 - accuracy: 0.9021 - val_loss: 0.2018 - val_accuracy: 0.9794\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3109 - accuracy: 0.9048 - val_loss: 0.1417 - val_accuracy: 0.9850\n",
      "Clean Test Accuracy: 98.50%\n",
      "Backdoor Success Rate: 11.21%\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_poisoned, y_train_poisoned, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate clean accuracy\n",
    "clean_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "print(f'Clean Test Accuracy: {clean_accuracy * 100:.2f}%')\n",
    "\n",
    "# Evaluate backdoor success rate\n",
    "x_test_triggered = add_trigger(x_test)\n",
    "y_test_targeted = to_categorical([target_label] * len(y_test), 10)\n",
    "backdoor_success_rate = model.evaluate(x_test_triggered, y_test_targeted, verbose=0)[1]\n",
    "print(f'Backdoor Success Rate: {backdoor_success_rate * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
